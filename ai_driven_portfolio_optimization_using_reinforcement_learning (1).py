# -*- coding: utf-8 -*-
"""AI-Driven Portfolio Optimization Using Reinforcement Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gdt8u8pEy81681kxFdPHoEF1R8hVShKa
"""

pip install "shimmy>=2.0"

pip install stable-baselines3

import pandas as pd
import numpy as np
import gym
from gym import spaces
from stable_baselines3 import PPO, DDPG, A2C
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

file_path = '/content/Portfolio_management.csv'
df = pd.read_csv(file_path)

date_columns = ["Report Date", "Publish Date", "Restated Date"]
for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors='coerce')

df = df.sort_values(by=["Ticker", "Report Date"])

financial_columns = ["Shares (Basic)", "Shares (Diluted)", "Revenue", "Cost of Revenue", "Gross Profit",
                     "Operating Expenses", "Selling, General & Administrative", "Research & Development",
                     "Depreciation & Amortization", "Operating Income (Loss)", "Non-Operating Income (Loss)",
                     "Interest Expense, Net", "Pretax Income (Loss), Adj.", "Net Income (Common)",
                     "Pretax Income (Loss)", "Income Tax (Expense) Benefit, Net", "Income (Loss) from Continuing Operations", "Net Income"]

df[financial_columns] = df.groupby("Ticker")[financial_columns].fillna(method="ffill")

df[financial_columns] = df[financial_columns].fillna(0)

df["Revenue Change"] = df.groupby("Ticker")["Revenue"].pct_change()

df["MA_5"] = df.groupby("Ticker")["Revenue Change"].transform(lambda x: x.rolling(window=5, min_periods=1).mean())
df["MA_20"] = df.groupby("Ticker")["Revenue Change"].transform(lambda x: x.rolling(window=20, min_periods=1).mean())
df["MA_50"] = df.groupby("Ticker")["Revenue Change"].transform(lambda x: x.rolling(window=50, min_periods=1).mean())

df["Volatility_20"] = df.groupby("Ticker")["Revenue Change"].transform(lambda x: x.rolling(window=20, min_periods=1).std())

df["Sharpe_Ratio"] = df["MA_20"] / (df["Volatility_20"] + 1e-6)

df["Momentum"] = df.groupby("Ticker")["Revenue"].transform(lambda x: x.diff(periods=10))

df = df.dropna(subset=["Revenue Change", "MA_5", "MA_20", "MA_50", "Volatility_20", "Sharpe_Ratio", "Momentum"])

scaler = StandardScaler()
df[["MA_5", "MA_20", "MA_50", "Volatility_20", "Sharpe_Ratio"]] = scaler.fit_transform(
    df[["MA_5", "MA_20", "MA_50", "Volatility_20", "Sharpe_Ratio"]]
)

class PortfolioEnv(gym.Env):
    def __init__(self, df, tickers, window_size=20):
        super(PortfolioEnv, self).__init__()
        self.df = df
        self.tickers = tickers
        self.window_size = window_size
        self.current_step = 0


        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(len(tickers), 5), dtype=np.float32
        )

        self.action_space = spaces.Box(
            low=0, high=1, shape=(len(tickers),), dtype=np.float32
        )

    def reset(self):
        self.current_step = self.window_size
        return self._next_observation()

    def _next_observation(self):
        obs = []
        for ticker in self.tickers:
            ticker_data = self.df[self.df["Ticker"] == ticker]

            if self.current_step >= len(ticker_data):
                obs.append([0, 0, 0, 0, 0])
            else:
                data = ticker_data.iloc[self.current_step]
                obs.append([
                    data["MA_5"],
                    data["MA_20"],
                    data["MA_50"],
                    data["Volatility_20"],
                    data["Sharpe_Ratio"]
                ])

        return np.array(obs, dtype=np.float32)

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.df)

        if done:
            reward = 0
        else:
            portfolio_return = 0
            for i, ticker in enumerate(self.tickers):
                ticker_data = self.df[self.df["Ticker"] == ticker]
                if self.current_step < len(ticker_data):
                    revenue_change = ticker_data.iloc[self.current_step]["Revenue Change"]
                    portfolio_return += action[i] * revenue_change

            reward = portfolio_return

        return self._next_observation(), reward, done, {}

tickers = df["Ticker"].unique()[:5]
env = PortfolioEnv(df, tickers)

ppo_model = PPO("MlpPolicy", env, verbose=1)
ppo_model.learn(total_timesteps=50000)

ddpg_model = DDPG("MlpPolicy", env, verbose=1)
ddpg_model.learn(total_timesteps=50000)

a2c_model = A2C("MlpPolicy", env, verbose=1)
a2c_model.learn(total_timesteps=50000)

def evaluate_agent(env, model, episodes=10):
    rewards = []
    for _ in range(episodes):
        obs = env.reset()
        done = False
        total_reward = 0
        while not done:
            action, _ = model.predict(obs)
            obs, reward, done, _ = env.step(action)
            total_reward += reward
        rewards.append(total_reward)
    return rewards

ppo_rewards = evaluate_agent(env, ppo_model)
ddpg_rewards = evaluate_agent(env, ddpg_model)
a2c_rewards = evaluate_agent(env, a2c_model)

print(f"PPO Average Reward: {np.mean(ppo_rewards)}")
print(f"DDPG Average Reward: {np.mean(ddpg_rewards)}")
print(f"A2C Average Reward: {np.mean(a2c_rewards)}")

plt.plot(ppo_rewards, label="PPO")
plt.plot(ddpg_rewards, label="DDPG")
plt.plot(a2c_rewards, label="A2C")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("Portfolio Optimization RL Agent Performance")
plt.legend()
plt.show()

def get_user_input(tickers):
    user_allocation = []
    print("Please enter your current portfolio allocation as percentages (e.g., 20 for 20%)")
    for ticker in tickers:
        while True:
            try:
                allocation = float(input(f"Enter allocation for {ticker}: ")) / 100
                if 0 <= allocation <= 1:
                    user_allocation.append(allocation)
                    break
                else:
                    print("Please enter a valid percentage between 0 and 100.")
            except ValueError:
                print("Invalid input. Please enter a valid number.")
    return np.array(user_allocation, dtype=np.float32)

def advise_user(env, model, user_allocation):
    obs = env.reset()
    model_suggestion, _ = model.predict(obs)

    print("\nUser's Current Allocation:")
    for ticker, alloc in zip(tickers, user_allocation):
        print(f"{ticker}: {alloc * 100:.2f}%")

    print("\nModel's Suggested Allocation:")
    for ticker, alloc in zip(tickers, model_suggestion):
        print(f"{ticker}: {alloc * 100:.2f}%")

    print("\nAdvice: Compare your current allocation with the model's suggestion. Consider adjusting your weights to achieve better portfolio optimization.")

def advise_user(env, model, user_allocation):
    obs = env.reset()
    model_suggestion, _ = model.predict(obs)

    user_allocation /= np.sum(user_allocation)
    model_suggestion /= np.sum(model_suggestion)

    similarity_score = np.dot(user_allocation, model_suggestion) * 10
    rating = min(10, round(similarity_score, 1))

    print("\nUser's Current Allocation:")
    for ticker, alloc in zip(tickers, user_allocation):
        print(f"{ticker}: {alloc * 100:.2f}%")

    print("\nModel's Suggested Allocation:")
    for ticker, alloc in zip(tickers, model_suggestion):
        print(f"{ticker}: {alloc * 100:.2f}%")

    print(f"\nYour Portfolio Rating: {rating}/10")

    print("\nSuggestions for Improvement:")
    for i, (ticker, user_alloc, model_alloc) in enumerate(zip(tickers, user_allocation, model_suggestion)):
        difference = model_alloc - user_alloc
        if difference > 0.05:
            print(f"- Consider increasing your allocation in {ticker} by approximately {difference * 100:.2f}%.")
        elif difference < -0.05:
            print(f"- Consider decreasing your allocation in {ticker} by approximately {-difference * 100:.2f}%.")
        else:
            print(f"- Your allocation in {ticker} is close to optimal.")